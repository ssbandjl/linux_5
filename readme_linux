https://docs.kernel.org/driver-api/scsi.html
https://github.com/ssbandjl/linux.git
bio下发流程: https://blog.csdn.net/flyingnosky/article/details/121362813
io路径: https://zhuanlan.zhihu.com/p/545906763
用户态与内核态通信netlink: https://gist.github.com/lflish/15e85da8bb9200794255439d0563b195

dmesg -w

git tag -d v5.10

nvme
static struct blk_mq_ops nvme_mq_ops -> struct blk_mq_ops 
static const struct blk_mq_ops bsg_mq_ops = {
	.queue_rq		= bsg_queue_rq,
	.init_request		= bsg_init_rq,
	.exit_request		= bsg_exit_rq,
	.complete		= bsg_complete,
	.timeout		= bsg_timeout,
};

驱动目录: drivers/nvme/Makefile
参考: https://mp.weixin.qq.com/s/GKVIY_NOXDfUCOho4zBMzw
Kconfig 文件的作用是：
1.控制make menuconfig时， 出现的配置选项；
2.根据用户配置界面的选择，将配置结果保存在.config 配置文件（该文件将提供给Makefile使用，用以决定要编译的内核组件以及如何编译）

drivers/nvme/host/core.c
module_init(nvme_core_init) -> static int __init nvme_core_init(void)
alloc_chrdev_region nvme
class_create("nvme")

drivers/nvme/target/rdma.c
static int __init nvmet_rdma_init(void)
ib_register_client -> drivers/infiniband/core/device.c -> int ib_register_client -> IB 驱动程序的上层用户可以使用 ib_register_client() 来注册 IB 设备添加和删除的回调。 添加 IB 设备时，将调用每个已注册客户端的 add 方法（按照客户端注册的顺序），而当删除设备时，将调用每个客户端的 remove 方法（按照客户端注册的相反顺序）。 另外，调用ib_register_client()时，客户端会收到所有已经注册的设备的add回调
nvmet_register_transport -> int nvmet_register_transport
	down_write
	up_write

EXPORT_SYMBOL(ib_register_client) 导出符号表

multi_path:
drivers/md/md.c
drivers/md/md-multipath.c


.map = multipath_map_bio -> static int multipath_map_bio

static void process_queued_bios

dm-mpath.c -> static int fail_path #路径故障
	pgpath->pg->ps.type->fail_path(&pgpath->pg->ps, &pgpath->path)
	dm_path_uevent(DM_UEVENT_PATH_FAILED -> 此补丁添加了对失败路径和恢复路径的 dm_path_event 的调用 -> void dm_path_uevent
		dm_build_path_uevent
		dm_uevent_add -> void dm_uevent_add -> list_add(elist, &md->uevent_list) -> 挂链表
		dm_send_uevents -> void dm_send_uevents
			dm_copy_name_and_uuid
			kobject_uevent_env 发送uevent, kobject_uevent这个函数原型如下，就是向用户空间发送uevent，可以理解为驱动（内核态）向用户（用户态）发送了一个KOBJ_ADD
				kobject_uevent_net_broadcast -> 参考热拔插原理
	queue_work
	schedule_work
static int __multipath_map_bio


热拔插: 热插拔：在不重启系统的情况下，增减硬件设备。本文主要介绍linux下的热插拔 https://www.cnblogs.com/tzj-kernel/p/15307231.html
热插拔：实现了驱动向用户态通知设备插拔
（1）外设插入，硬件中断响应
（2）总线发现新的设备，驱动probe 再调用device_add(设备驱动？？)
（3）device_add调用kobject_uevent(, KOBJ_ADD)，向用户空间广播新设备加入事件通知；这里发出通知的方式，就是netlink；
（4）用户空间运行的daemon(udev)收到event事件广播；
（5）udev根据消息和环境变量，查询sysfs中的/sys的变化，按照规则(/etc/udev/rules.d/*)，在/dev目录下自动创建设备节点；
（6）运行 /sbin/hotplug 脚本
kobject_uevent_env--》kobject_uevent_net_broadcast--》uevent_net_broadcast_untagged或者uevent_net_broadcast_tagged--》netlink_broadcast，实际上就是将buf中的内容发送到用户空间，用户空间udev监听到此消息，则解析
/sys/kernel/uevent_helper

内核 uevents 和 udev 
必需的设备信息由 sysfs 文件系统导出。对于内核检测到并已初始化的设备，将创建一个带有该设备名称的目录。它包含带有特定于设备属性的属性文件。
每次添加或删除设备时，内核都会发送 uevent 来向 udev 通知更改。一旦启动，udev 守护程序便会读取并分析 /usr/lib/udev/rules.d/*.rules 和 /etc/udev/rules.d/*.rules 文件中的所有规则，并将它们保留在内存中。如果更改、添加或去除了规则文件，守护程序可以使用 udevadm control --reload 命令重新装载这些规则在内存中的表示。有关 udev 规则及其语法的更多细节，请参见第 22.6 节 “使用 udev 规则影响内核设备事件处理”。每个接收到的事件都根据所提供的规则集进行匹配。这些规则可以增加或更改事件环境键、为要创建的设备节点请求特定名称、添加指向该节点的符号链接或者添加设备节点创建后运行的程序。从内核 netlink 套接字接收驱动程序核心 uevent
事件处理: https://documentation.suse.com/zh-cn/sles/15-SP1/html/SLES-all/cha-udev.html
udev rule: ls -alh /usr/lib/udev/rules.d/
cat /usr/lib/udev/rules.d/62-multipath.rules




uevent设计: https://github.com/ssbandjl/linux/commit/7a8c3d3b92883798e4ead21dd48c16db0ec0ff6f
device-mapper uevent代码为device-mapper增加了创建和发送kobject uevents（uevents）的能力。 以前的设备映射器事件只能通过 ioctl 接口获得。 uevents 接口的优点是事件包含环境属性，为事件提供增加的上下文，避免在收到事件后查询设备映射器设备的状态
由 udevmonitor 捕获的生成的 uevent 示例


insert_work(pwq, work, worklist, work_flags)

dm_init_init
	dm_early_create
__bind
dm_table_event_callback(t, event_callback, md)
event_fn
dm_table_event
trigger_event

__map_bio

io_path, io路径
static const struct block_device_operations dm_blk_dops = {
	.submit_bio = dm_submit_bio,  -> static void dm_submit_bio

dm_split_and_process_bio
	__split_and_process_bio 选择正确的策略来处理非flush bio
		dm_table_find_target
		__map_bio
			ti->type->map(ti, clone) -> .map = multipath_map_bio -> static int __multipath_map_bio -> bio_set_dev(bio, pgpath->path.dev->bdev) -> pgpath->pg->ps.type->start_io
	bio_trim
	trace_block_split
	bio_inc_remaining
	submit_bio_noacct -> void submit_bio_noacct(struct bio *bio) ->  为 I/O 重新提交 bio 到块设备层 bio 描述内存和设备上的位置。 这是 submit_bio() 的一个版本，只能用于通过堆叠块驱动程序重新提交给较低级别驱动程序的 I/O。 块层的所有文件系统和其他上层用户应该使用 submit_bio() 代替, bio 在节流之前已经被检查过，所以在从节流队列中调度它之前不需要再次检查它。 为此目的添加 submit_bio_noacct_nocheck() 的助手
		__submit_bio_noacct_mq
		submit_bio_noacct_nocheck -> submit_bio() 确实可以通过递归调用 submit_bio_noacct 添加更多的 bios
			__submit_bio
				blk_mq_submit_bio -> void blk_mq_submit_bio -> 向块设备提交bio, 会执行调度,合并等操作
					blk_mq_bio_to_request
					blk_queue_bounce -> 弹跳
					__bio_split_to_limits
					blk_mq_bio_to_request -> request
					blk_mq_insert_request
						blk_mq_request_bypass_insert
					blk_mq_run_hw_queue

blk_mq_plug_issue_direct
blk_add_rq_to_plug

blk_mq_requeue_request
	blk_mq_sched_requeue_request
		e->type->ops.requeue_request(rq) -> requeue_request = 

blk_mq_submit_bio
blk_mq_run_hw_queue
__blk_mq_sched_dispatch_requests

static const struct blk_mq_ops nvme_rdma_mq_ops = {
	.queue_rq	= nvme_rdma_queue_rq,
	.complete	= nvme_rdma_complete_rq,
	.init_request	= nvme_rdma_init_request,
	.exit_request	= nvme_rdma_exit_request,
	.init_hctx	= nvme_rdma_init_hctx,
	.timeout	= nvme_rdma_timeout,
	.map_queues	= nvme_rdma_map_queues,
	.poll		= nvme_rdma_poll,
};
nvme_rdma_queue_rq
drivers/nvme/host/rdma.c -> static int nvme_rdma_post_send
	ib_post_send


三条链：current->bio_list存储在当前线程的所有bio; plug->mq_list使能plug/unplug机制时存放在缓存池的bio；若定义IO调度层，IO请求会发送到scheduler list中；若没有定义IO调度层，IO请求会发送到ctx->rq_lists
每个线程若已经在执行blk_mq_submit_bio()，将新下发BIO链入到线程current->bio_list;
依次处理current->list中的每个bio；
若bio中存在数据在高端内存区，在低端内存区分配内存，将高端内存区数据拷贝到新分配的内存区，称为bounce过程，后面单独一节介绍；
检查请求队列中的bio，若过大进行切分，称BIO的切分；
尝试将bio合并到plug->mq_list中，然后尝试合并到IO调度层链表或ctx->rq_lists中；
若没有合并，分配新的request；
若定义plug，且没达到冲刷数目，加入到plug->mq_list；若达到冲刷数目，将冲刷下发（plug/unplug机制）；
若定义IO调度器，往IO调度器中插入新的request（对于机械硬盘，通过IO调度层座合并和排序，有利于提高性能）;
若 没有定义IO调度器，可以直接下发（对于较快的硬盘如nvme盘，进入调度层可能会浪费时间，跳过IO调度层有利于性能提升）
（1）bounce过程
（2）bio的切分和合并
（3）IO请求和tag的分配
（4）plug/unplug机制
（5）IO调度器
（4）其他


app读写io:
int main()
{
       char buff[128] = {0};
       int fd = open("/var/pilgrimtao.txt", O_CREAT|O_RDWR);
​
       write(fd, "pilgrimtao is cool", 18);
       pread(fd, buff, 128, 0);
       printf("%s\n", buff);
​
       close(fd);
       return 0;
}


文件系统:
SYSCALL_DEFINE3(read, ...) -> ksys_read -> vfs_read -> read_iter -> xfs_file_read_iter
SYSCALL_DEFINE3(write, ...) -> ksys_write -> vfs_write -> new_sync_write -> call_write_iter ->write_iter -> xfs_file_write_iter

磁盘(disk)的访问模式有三种 BUFFERED、DIRECT、DAX。前面提到的由于page cache存在可以避免耗时的磁盘通信就是BUFFERED访问模式的集中体现；但是如果我要求用户的write请求要实时存储到磁盘里，不能只在内存中更新，那么此时我便需要DIRECT模式；大家可能听说过flash分为两种nand flash和nor flash，nor flash可以像ram一样直接通过地址线和数据线访问，不需要整块整块的刷，对于这种场景我们采用DAX模式。所以file_operations的read_iter和write_iter回调函数首先就需要根据不同的标志判断采用哪种访问模式
kernel在2020年12月的patch中提出了folio的概念，我们可以把folio简单理解为一段连续内存，一个或多个page的集合，他和page的关系如图
代码参考：xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __filemap_get_folio -> filemap_alloc_folio

读取磁盘inode代码参考：iomap_file_buffered_write -> iomap_iter -> .iomap_begin -> xfs_buffered_write_iomap_begin -> xfs_iread_extents -> xfs_btree_visit_blocks -> xfs_btree_readahead_ptr -> xfs_buf_readahead -> xfs_buf_readahead_map -> xfs_buf_read_map -> xfs_buf_read -> xfs_buf_submit -> __xfs_buf_submit -> xfs_buf_ioapply_map -> submit_bio

代码参考： xfs_file_read_iter -> xfs_file_buffered_read -> generic_file_read_iter -> filemap_read -> filemap_get_pages -> filemap_create_folio -> filemap_alloc_folio -> folio_alloc
filemap_get_pages -> filemap_readahead -> page_cache_async_ra -> ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> filemap_alloc_folio/filemap_add_folio

xfs_file_read_iter -> xfs_file_buffered_read -> generic_file_read_iter -> filemap_read -> copy_folio_to_iter(offset)

filemap_get_pages -> filemap_readahead -> page_cache_async_ra -> ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> read_pages -> aops.readahead -> xfs_vm_readahead -> iomap_readahead -> iomap_iter -> ops.iomap_begin（xfs文件系统维护的回调函数）

iomap_readahead -> iomap_readahead_iter -> iomap_readpage_iter -> bio_alloc/bio_add_folio

代码参考：iomap_readahead -> iomap_iter -> ops.iomap_begin（xfs文件系统维护的回调函数）

代码参考：
sys_read：ondemand_readahead -> do_page_cache_ra -> page_cache_ra_unbounded -> xa_load（在sys_read流程中，因为一开始就会把所有的folio都拿到，不是一个一个拿的）
iomap_readahead_iter -> readahead_folio
sys_write：
xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __filemap_get_folio -> mapping_get_entry/filemap_add_folio （在sys_write流程，是用完一个folio，再申请新的folio）


代码参考：
sys_read：iomap_readpage_iter -> bio_add_folio -> __bio_try_merge_page
sys_write：xfs_file_write_iter -> xfs_file_buffered_write -> iomap_file_buffered_write -> iomap_write_iter -> iomap_write_begin -> __iomap_write_begin -> iomap_read_folio_sync -> bio_init/bio_add_folio （一个bio只有一个bio_vec）


bpf: bpf.h

syscall 系统调用: linux/syscalls.h


iscsi:
ko:
[root@n73 linux-5.10.182]# find . -name "*.ko"|grep scsi
./drivers/firmware/iscsi_ibft.ko
./drivers/message/fusion/mptscsih.ko
./drivers/scsi/aacraid/aacraid.ko
./drivers/scsi/aic7xxx/aic79xx.ko
./drivers/scsi/arcmsr/arcmsr.ko
./drivers/scsi/be2iscsi/be2iscsi.ko
./drivers/scsi/bfa/bfa.ko
./drivers/scsi/bnx2fc/bnx2fc.ko
./drivers/scsi/bnx2i/bnx2i.ko
./drivers/scsi/csiostor/csiostor.ko
./drivers/scsi/cxgbi/cxgb3i/cxgb3i.ko
./drivers/scsi/cxgbi/cxgb4i/cxgb4i.ko
./drivers/scsi/cxgbi/libcxgbi.ko
./drivers/scsi/fcoe/fcoe.ko
./drivers/scsi/fcoe/libfcoe.ko
./drivers/scsi/fnic/fnic.ko
./drivers/scsi/isci/isci.ko
./drivers/scsi/libfc/libfc.ko
./drivers/scsi/libsas/libsas.ko
./drivers/scsi/lpfc/lpfc.ko
./drivers/scsi/megaraid/megaraid_sas.ko
./drivers/scsi/mpt3sas/mpt3sas.ko
./drivers/scsi/mvsas/mvsas.ko
./drivers/scsi/pm8001/pm80xx.ko
./drivers/scsi/qedf/qedf.ko
./drivers/scsi/qedi/qedi.ko
./drivers/scsi/qla2xxx/qla2xxx.ko
./drivers/scsi/qla2xxx/tcm_qla2xxx.ko
./drivers/scsi/smartpqi/smartpqi.ko
./drivers/scsi/ufs/ufshcd-core.ko
./drivers/scsi/ufs/ufshcd-pci.ko
./drivers/scsi/3w-9xxx.ko
./drivers/scsi/3w-sas.ko
./drivers/scsi/ch.ko
./drivers/scsi/hpsa.ko
./drivers/scsi/hv_storvsc.ko
./drivers/scsi/mvumi.ko
./drivers/scsi/pmcraid.ko
./drivers/scsi/libiscsi_tcp.ko
./drivers/scsi/scsi_debug.ko
./drivers/scsi/raid_class.ko
./drivers/scsi/scsi_transport_fc.ko
./drivers/scsi/scsi_transport_spi.ko
./drivers/scsi/scsi_transport_sas.ko
./drivers/scsi/hptiop.ko
./drivers/scsi/initio.ko
./drivers/scsi/iscsi_tcp.ko
./drivers/scsi/scsi_transport_srp.ko
./drivers/scsi/sd_mod.ko
./drivers/scsi/ses.ko
./drivers/scsi/sg.ko
./drivers/scsi/sr_mod.ko
./drivers/scsi/st.ko
./drivers/scsi/stex.ko
./drivers/scsi/virtio_scsi.ko
./drivers/scsi/vmw_pvscsi.ko
./drivers/target/iscsi/cxgbit/cxgbit.ko
./drivers/target/iscsi/iscsi_target_mod.ko
./drivers/target/target_core_pscsi.ko

磁盘驱动sd: ./drivers/scsi/sd_mod.ko
通用驱动sg: ./drivers/scsi/sg.ko

drivers/scsi/sd.h
include/scsi/scsi_host.h
struct scsi_host_template, Let it rip 分叉, 
drivers/scsi/scsi.c -> init -> static int __init init_scsi -> 驱动加载
	scsi_init_devinfo
	scsi_init_hosts
	scsi_sysfs_register
	scsi_netlink_init


static struct pci_driver isci_pci_driver = {
	.name		= DRV_NAME,
	.id_table	= isci_id_table,
	.probe		= isci_pci_probe,
	.remove		= isci_pci_remove,
	.driver.pm      = &isci_pm_ops,
};
isci_pci_probe
isci_host_alloc
scsi_host_alloc -> struct Scsi_Host *scsi_host_alloc
	scsi_error_handler -> int scsi_error_handler -> 错误恢复
		scsi_restart_operations(shost) -> static void scsi_restart_operations

int iscsi_host_add -> 模块加载
	scsi_add_host -> scsi_add_host_with_dma 

scsi_remove_host -> 模块卸载

Linux打印内核函数调用栈 dump_stack, krpobe, systemtap, ftrace, qemu等等, bpftrace, perf调性能


void blk_finish_plug
bpftrace -e 'tracepoint:block:block_rq_insert { printf("Block I/O by %s\n", kstack); }'
Block I/O by 
        __elv_add_request+259
        blk_flush_plug_list+320
        blk_finish_plug+20
        _xfs_buf_ioapply+820
        __xfs_buf_submit+114
        xlog_bdstrat+55
        xlog_sync+742
        xlog_state_release_iclog+123
        xfs_log_force_lsn+497
        xfs_file_fsync+253
        do_fsync+85
        sys_fsync+16
        system_call_fastpath+37

lsmod|grep sd_mod
modinfo sd_mod
modinfo drivers/scsi/sd_mod.ko
make clean; make; rmmod sd_mod.ko; insmod sd_mod.ko

bpftrace -l | more

#!/usr/local/bin/bpftrace
tracepoint:syscalls:sys_enter_nanosleep
{
  printf("%s is sleeping.\n", comm);
}

bpftrace -e 'kprobe:do_sys_open { printf("opening: %s\n", str(arg1)); }'
bpftrace -e 'kprobe:ip_output { @[kstack] = count(); }'

bpftrace -e 'kprobe:scsi_error_handler { printf("bt:%s\n", kstack); }'

动态跟踪事件: bpftrace -e 'kprobe:scsi_host_set_state { printf("bt:%s\n", kstack); }'
bt:
        scsi_host_set_state+1
        scsi_remove_host+155
        iscsi_host_remove+120
        iscsi_sw_tcp_session_destroy+85 -> .destroy_session	= iscsi_sw_tcp_session_destroy
        iscsi_if_recv_msg+3382 -> ISCSI_UEVENT_DESTROY_SESSION
        iscsi_if_rx+202
        netlink_unicast+421
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68
				
断开会话:
iscsiadm -m node --logoutall=all
ISCSI_UEVENT_DESTROY_SESSION
transport->destroy_session(session) -> iscsi_sw_tcp_session_destroy
	if (scsi_host_set_state(shost, SHOST_CANCEL))

rfc:
7. iSCSI Error Handling and Recovery ..............................92
bpftrace -e 't:syscalls:sys_enter_execve { printf("%s called %s\n", comm, str(args->filename)); }'

查看所有事件: bpftrace -l|grep scsi

scsi_scan_host -> void scsi_scan_host

sd_spinup_disk
	scsi_execute_cmd TEST_UNIT_READY START_UNIT

scsi_execute_req 
blk_execute_rq -> blk_status_t blk_execute_rq -> 底层调用高层(scci -> blk)


错误恢复:
● int (* eh_abort_handler) (struct scsi_cmnd *);
● int (* eh_device_reset_handler) (struct scsi_cmnd *);
● int (* eh_target_reset_handler) (struct scsi_cmnd *);
● int (* eh_bus_reset_handler) (struct scsi_cmnd *);
● int (* eh_host_reset_handler) (struct scsi_cmnd *);

scsi_eh_scmd_add
scsi_eh_abort_cmds
scsi_eh_flush_done_q

__blk_run_queue
scsi_send_eh_cmnd

查看会话: iscsiadm --mode session
连接: iscsiadm -m node -T iqn.2018-01.com.h3c.onestor:622a4b9193bb4ee68cdb6cae6b76cc74 -p 172.16.156.3:3260 -l
bt:
        scsi_host_set_state+1
        scsi_add_host_with_dma.cold.9+294
        iscsi_sw_tcp_session_create+130
        iscsi_if_create_session+48 -> iscsi_if_create_session(struct iscsi_internal *priv  -> ISCSI_UEVENT_CREATE_SESSION
        iscsi_if_recv_msg+1109 -> iscsi_if_recv_msg(struct sk_buff *skb
        iscsi_if_rx+202 -> .input	= iscsi_if_rx
        netlink_unicast+421 -> int netlink_unicast
        netlink_sendmsg+539
        sock_sendmsg+91
        ____sys_sendmsg+451
        ___sys_sendmsg+124
        __sys_sendmsg+87
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68


netlink_kernel_create(&init_net, NETLINK_ISCSI, &cfg) -> netlink_kernel_create(struct net *net, int unit, struct netlink_kernel_cfg *cfg) ->input：为内核模块定义的netlink消息处理函数，当有消 息到达这个netlink socket时，该input函数指针就会被引用，且只有此函数返回时，调用者的sendmsg才能返回
	nlk_sk(sk)->netlink_rcv = cfg->input -> nlk->netlink_rcv(skb) <- static int netlink_unicast_kernel


netlink_unicast
	netlink_unicast_kernel


Attaching 1 probe...
bt:
        fail_path+1
        multipath_message+342 -> .message = multipath_message
        target_message+627
        ctl_ioctl+431
        dm_ctl_ioctl+10
        __x64_sys_ioctl+132
        do_syscall_64+51
        entry_SYSCALL_64_after_hwframe+68

int dm_register_target(struct target_type *tt)

static ioctl_fn lookup_ioctl
{DM_TARGET_MSG_CMD, 0, target_message} -> static int target_message
	ti->type->message(ti, argc, argv, result, maxlen) -> static int multipath_message
		action = fail_path;
		action_dev(m, dev, action) -> static int action_dev
			action(pgpath)


Documentation/kbuild/modules.rst

cp /lib/modules/`uname -r`/build/Makefile ./Makefile_current

CONFIG_PREEMPT_BUILD=y
此选项通过使所有内核代码（不在关键部分中执行）可抢占来减少内核的延迟。 这允许通过允许低优先级进程被非自愿地抢占来对交互事件做出反应，即使它在内核模式下执行系统调用，否则不会到达自然抢占点。 这允许应用程序即使在系统处于负载下时也能更“流畅”地运行，但代价是吞吐量略低，内核代码的运行时开销也很小。
如果您正在为延迟要求在毫秒范围内的桌面或嵌入式系统构建内核，请选择此选项

编译指定模块: 
cd drivers/scsi/
make -C /lib/modules/$(uname -r)/build M=$(pwd) modules

.config -> CONFIG_SYSTEM_TRUSTED_KEYS
make menuconfig

include/config/kernel.release
include/generated/utsrelease.h
include/linux/vermagic.h